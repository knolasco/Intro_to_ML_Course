{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and MNIST\n",
    "\n",
    "Kevin Nolasco\n",
    "\n",
    "Cabrini University\n",
    "\n",
    "MCIS560 - Intro to Machine Learning\n",
    "\n",
    "02/27/2022\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this module is to get hands on experience building and tweaking a neural network using tensorflow. We will apply what we have learned about Neural Networks to the MNIST dataset. Our goal is to build a model that will have >98% accuracy on the test set.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Creating a model using tf.keras is simple and powerful. It is very important to pay attention to the learning rates, batch_size, and number of epochs used when training a neural network. It is also important to process the data correctly, so that we don't run into the issue of exploding gradients and get nan results for the loss and accuracy.\n",
    "\n",
    "After carefully tuning the model, I was able to attain an accuracy of 98.12% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Load the data. See what type is the data to know which methods are available and to learn more about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply DropOut to Images\n",
    "I will use the same technique that I used in Module 3; use a threshold change the data to binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "def reshape_X_and_y(X, y):\n",
    "    return X.reshape(X.shape[0], -1), y.reshape(y.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = reshape_X_and_y(X_train, y_train)\n",
    "X_test, y_test = reshape_X_and_y(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.3813775510204\n"
     ]
    }
   ],
   "source": [
    "# find threshold\n",
    "row_means = X_train.mean(axis = 1)\n",
    "threshold = row_means.max()\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectorized_threshold = np.vectorize(lambda val: 0 if val < threshold else 1)\n",
    "\n",
    "X_train_dropout = vectorized_threshold(X_train)\n",
    "X_test_dropout = vectorized_threshold(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take an example in the training set and visualize it's dropout counter-part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAEtCAYAAADHtl7HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaBUlEQVR4nO3de7BddZnm8echwKghAukc0yEdSDcylClqDPYRbEEMYKNty62qx4ZmwqXkMlVctDopBOM06RkdU9hqt+A4FZAJKKAWkpaMzIyJQwwXJ5MTTYcgZLhMwISQHEyDEZzh9s4fe6Vnc8w+v/Xbe+3Lyvl+qqyzz9rvWes92+yXZ6+99u84IgQAAIDy9ut3AwAAAHVDgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIA1IrtD9jePM79s22H7f172RcmFgLUBGB7i+2dtic3bbvY9uoO9vcb27ttv2D7Idv/2vbA/XtikAKDz/a1tv/LmG2Pt9h2TkTcHxFHN23fYvtDHRx/me1Xipm22/Ym21+wfXC7++ymTn9fVGPg/oOHrpkk6ZMV7u/0iJgi6QhJSyR9WtI3WhXbnlThsQHsW9ZIev+eOWF7hqQDJB07Zts7i9puuL6YaUOSLpL0PkkPNr/wbMaLMhCgJo4vSlpo+5C93Wn7/bbX2X6x+Pr+MjuNiBcj4h5Jfy7pAtvHFPtbZvvrtu+1/ZKkk22/y/bq4qzVI7bPaDr+Mtv/0fbK4hXgj20fUaa/sa/GbC+2/a3i2z3D9gXbv7b9R6UeLQC9tE6NwDS3+P4Dku6TtHnMticj4lnb82xvlSTb35R0uKQVxXP86qb9nmf7GdvP215UppGI+D8RsU7SGZJ+R40wJdsX2n7Q9lds/1LSYtsH277N9qjtp21/ds+Z+Kb6G4u59ZjtU/ccx/Zhtu+xvcv2E7Yvabpvme3PNX1f9vdFDxGgJo4RSaslLRx7h+2pkn4g6atqDIwvS/qB7d8pu/OI+J+Stqox5Pb4C0mflzRF0lpJKyT9UNI7JF0p6XbbRzfVnyfp30maJmmDpNsr6O+k4ushEXFQRPyk7O8EoDci4hU1ZsSe5+tJku6X9MCYbb919iki5kt6Ro2z4gdFxPVNd58o6WhJp0r6K9vvyuhpt6SVevNMO17SU5KmqzHbbpB0sKQ/kPRBSeerCFxN9U+qMdOuk3R3Mc8k6dtqzMzDJP2ZpH9v+5QSfY33+6KHCFATy19JutL20Jjtfyrp8Yj4ZkS8FhF3SnpM0umZ+39W0tSm778fEQ9GxBtqvIo8SNKSiHglIv67pP8s6dym+h9ExJqI+L+SFkn6I9uzKuwPwOD6sf5/WPqAGgHq/jHbfpy5z7+OiN9ExD9I+gdJ7878+bEz7dmIuCEiXpP0iqRzJF0bEbsjYoukL0ma31S/U9LfRsSrEfEdNc6o/Wkx106Q9OnijNcGSTerEcBQEwSoCSQiNqkRWq4Zc9dhkp4es+1pSTMzDzFT0q6m738x5hi/KMJUq2P8U31E/LrY12EV9gdgcK2RdGJxhmYoIh6X9JAa10ZNlXSM8q9/eq7p9stqvIjLMd5Mm6bG247Ns2nsXNoWETHm/j0zbVdxlqvVz2LAEaAmnuskXaI3P1GfVeNi8GaHS9pWdqe231vs84Gmzc2D41lJs8Z8Um/sMWY17e8gNV75PVuiv5ckva3pvt9t0QOAwfUTNd4Ou0TSg5IUEb9S4/l/iRpnf/53i5+t/HlezKAPqXEWbG/HeV7Sq3rzbBo702ba9pj798y0qbantPjZ8Wba2D7QJwSoCSYinpD0HUlXNW2+V9I/t/0Xtve3/eeS5qhxtmpctt9u+2NqvJ//rYh4uEXpWjVeAV5t+wDb89R4C+7bTTUftX2i7QPVuBbqf0TEL0r0t0HSOcV+h9W4nmCPUUlvqHGNAoABFRG/UeNazb/Um0PLA8W28c4+7VBFz3Hb/8z2H0r6e0n/KOk/tej3dUnflfR521OKD738paRvNZW9Q9JVxWz6l5LeJeneYq49JOkLtt9i+19I+kTTz25QYx5Otf27kj415vCV/b5oHwFqYvq3kv7po7kR8UtJH5O0QNIvJV0t6WMR8fw4+1hhe7cap7QXqXFh90WtiouLRE+X9CdqvHL7D5LOj4jHmsruUOMM2S5JfyjpX5Xs799IOlKNYffXxX72HPdlNS72fLD49N/7xvmdAPTXj9UIHc1nsu8vto0XoL4g6bPFc/y3PihT0tXFTPulpNskrZf0/oh4aZyfuVKNs0VPFT3fIemWpvvXSjpKjZn3eUl/VswzqXH952w1zkYtl3RdRKwq7vumGtdsbVHjgzffGXPcKn5fdMhvfnsW6A/byyRtjYjP9rsXAOiU7QslXRwRJ/a7F3QHZ6AAAAAyEaAAAAAy8RYeAABAJs5AAQAAZCJAAQAAZOrpX5OeNm1azJ49u5eHBNBn69evfz4ixv75oNphfgETz3jzq6MAZfsjkv5O0iRJN0fEkvHqZ8+erZGRkU4OCaBmbI/9MzwDI2eGMb+AiWe8+dX2W3i2J0n6mhoLI86RdK7tOe3uDwB6iRkGoBOdXAN1nKQnIuKpYpXpb0s6s5q2AKDrmGEA2tZJgJqpN/9l6q3iL0kDqA9mGIC2df1TeLYvtT1ie2R0dLTbhwOAyjC/ALTSSYDaJmlW0/e/V2x7k4hYGhHDETE8NFT7D+IA2HckZxjzC0ArnQSodZKOsv37tg+UdI6ke6ppCwC6jhkGoG1tL2MQEa/ZvkLSf1PjI8C3RMQjlXUGAF3EDAPQiY7WgYqIeyXdW1EvANBTzDAA7eJPuQAAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGTav98NYLC9/vrryZoXX3yxB5003Hjjjcmal19+OVmzefPmZM3Xvva1ZM3ChQuTNXfeeWeyRpLe8pa3JGuuueaaZM11111X6ngAest2z44VEcmaXvZTVpm+B0VHAcr2Fkm7Jb0u6bWIGK6iKQDoBWYYgHZVcQbq5Ih4voL9AEA/MMMAZOMaKAAAgEydBqiQ9EPb621fWkVDANBDzDAAben0LbwTI2Kb7XdIWmn7sYhY01xQDKVLJenwww/v8HAAUKlxZxjzC0ArHZ2BiohtxdedkpZLOm4vNUsjYjgihoeGhjo5HABUKjXDmF8AWmk7QNmebHvKntuSTpO0qarGAKCbmGEAOtHJW3jTJS0v1pHYX9IdEfFfK+kKALqPGQagbW0HqIh4StK7K+wFkp555plkzSuvvJKseeihh5I1DzzwQLLmhRdeSNbcddddyZpBM2vWrGTNlVdemaxZvnx5smbKlCmlenr3u9NPpw9+8IOl9oU0Zli9DeIikINkEB+fOi2SWQbLGAAAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGQiQAEAAGTq9I8Jo6Sf/exnpepOOeWUZM2LL77YaTv7tEmTJiVrPve5zyVrJk+enKw577zzkjWHHXZYskaSDj300GTN0UcfXWpfQD8M4uKN+6J9bUHKuuIMFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCYCFAAAQCZWIu+RI444olTdtGnTkjV1XIn8+OOPT9aUWYn7vvvuS9YceOCByZr58+cnawAAaIUzUAAAAJkIUAAAAJkIUAAAAJkIUAAAAJkIUAAAAJkIUAAAAJkIUAAAAJkIUAAAAJlYSLNHpk6dWqrui1/8YrJmxYoVyZpjjz02WXPVVVeV6ill7ty5yZpVq1YlayZPnpys2bRpU7Lmq1/9arIGQPUiIlljuwedVKvM71WVOj4+ExVnoAAAADIRoAAAADIRoAAAADIRoAAAADIRoAAAADIRoAAAADIRoAAAADIRoAAAADIlF9K0fYukj0naGRHHFNumSvqOpNmStkj6eET8Y/fanDjOOuusZM0pp5ySrJkyZUqyZuPGjcmam2++OVmzcOHCZE2ZRTLLOOaYY5I1S5cureRY2DcwwwZLVYtSVrXgZC8XySxj0PpBa2XOQC2T9JEx266R9KOIOErSj4rvAWAQLRMzDEDFkgEqItZI2jVm85mSbi1u3yrprGrbAoBqMMMAdEO710BNj4jtxe3nJE2vqB8A6AVmGICOdHwReTTesG35pq3tS22P2B4ZHR3t9HAAUKnxZhjzC0Ar7QaoHbZnSFLxdWerwohYGhHDETE8NDTU5uEAoFKlZhjzC0Ar7QaoeyRdUNy+QNL3q2kHAHqCGQagI8kAZftOST+RdLTtrbY/IWmJpD+2/bikDxXfA8DAYYYB6IbkOlARcW6Lu06tuBcAqBwzDEA3JAMUBs/b3/72SvZz8MEHV7KfMottnnPOOcma/fZjYXwAvVVmQU4Wt8Te8F8sAACATAQoAACATAQoAACATAQoAACATAQoAACATAQoAACATAQoAACATAQoAACATCykOYEtXrw4WbN+/fpkzerVq5M1q1atStacdtppyRoAkMotbllmkUygXZyBAgAAyESAAgAAyESAAgAAyESAAgAAyESAAgAAyESAAgAAyESAAgAAyESAAgAAyMRCmhPY5MmTkzU33XRTsuY973lPsuaSSy5J1px88snJmuHh4WTN5ZdfnqxhgT1g31fVYpu9nBdlesZg4AwUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJhbSxLiOPPLIZM2yZcuSNRdddFGy5rbbbquk5qWXXkrWnH/++cmaGTNmJGsA1FtVi21WpcyxWGxzMHAGCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMLaaJjZ599drLmne98Z7JmwYIFyZpVq1Yla6699tpkzdNPP52sWbRoUbJm5syZyRoA9cZim9ib5Bko27fY3ml7U9O2xba32d5Q/O+j3W0TANrDDAPQDWXewlsm6SN72f6ViJhb/O/eatsCgMosEzMMQMWSASoi1kja1YNeAKByzDAA3dDJReRX2N5YnB4/tLKOAKA3mGEA2tZugPq6pCMlzZW0XdKXWhXavtT2iO2R0dHRNg8HAJUqNcOYXwBaaStARcSOiHg9It6QdJOk48apXRoRwxExPDQ01G6fAFCZsjOM+QWglbYClO0ZTd+eLWlTq1oAGDTMMACdSq4DZftOSfMkTbO9VdJ1kubZnispJG2RdFn3WgSA9jHDAHSDe7nY1vDwcIyMjPTseKiXF154IVmzYsWKZM2FF16YrCnz7/7UU09N1qxcuTJZM9HZXh8Rw/3uo1PML/RCLxfkZLHNtPHmF3/KBQAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIFNyJXKgVw455JBkzfz585M1F198cbLm1VdfTdasWbMmWbN69epkzbx585I1AIB64QwUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJgIUAABAJhbSRE9s3LgxWXPXXXcla9atW5esKbNIZhlz5sxJ1px00kmVHAtAvdnudwvoMc5AAQAAZCJAAQAAZCJAAQAAZCJAAQAAZCJAAQAAZCJAAQAAZCJAAQAAZCJAAQAAZGIhTYxr8+bNyZobbrghWXP33Xcna5577rlSPVVh//3T//RnzJiRrNlvP16DAHXGAphoF9MfAAAgEwEKAAAgEwEKAAAgEwEKAAAgEwEKAAAgEwEKAAAgEwEKAAAgEwEKAAAgEwtp7qPKLEp5xx13JGtuvPHGZM2WLVvKtNQz733ve5M1ixYtStacccYZVbQDoAsm8gKYEdHvFqASZ6Bsz7J9n+2f237E9ieL7VNtr7T9ePH10O63CwDlMb8AdEuZt/Bek7QgIuZIep+ky23PkXSNpB9FxFGSflR8DwCDhPkFoCuSASoitkfET4vbuyU9KmmmpDMl3VqU3SrprC71CABtYX4B6Jasi8htz5Z0rKS1kqZHxPbiruckTa+2NQCoDvMLQJVKByjbB0n6nqRPRcSvmu+LxhVte72qzfaltkdsj4yOjnbULAC0g/kFoGqlApTtA9QYPrdHxN3F5h22ZxT3z5C0c28/GxFLI2I4IoaHhoaq6BkASmN+AeiGMp/Cs6RvSHo0Ir7cdNc9ki4obl8g6fvVtwcA7WN+AeiWMutAnSBpvqSHbW8otn1G0hJJ37X9CUlPS/p4VzoEgPYxvwB0RTJARcQDklqtWHZqte1gx44dyZpHHnkkWXPFFVckax577LFSPfXK8ccfn6y5+uqrkzVnnnlmsma//ViEfyJgftXTvrpIJgtg7lv4rwgAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAECmMn/KBQm7du1K1lx22WWl9rVhw4ZkzZNPPllqX71ywgknJGsWLFiQrPnwhz+crHnrW99aqicA1dlXVwYvixXEsTecgQIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMhEgAIAAMg0oRfSXLt2bbLm+uuvT9asW7cuWbN169ZSPfXS2972tmTNVVddlaxZtGhRsmby5MmlegJQnYm+AGYZLJKJdnEGCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAINOEXkhz+fLlldRUac6cOcma008/PVkzadKkZM3ChQuTNYccckiyBkDvTfRFMlkAE/3GGSgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMyYU0bc+SdJuk6ZJC0tKI+DvbiyVdImm0KP1MRNzbrUa7YcmSJZXUABhM+/L8YiFJoL/KrET+mqQFEfFT21Mkrbe9srjvKxHxN91rDwA6wvwC0BXJABUR2yVtL27vtv2opJndbgwAOsX8AtAtWddA2Z4t6VhJa4tNV9jeaPsW24dW3RwAVIX5BaBKpQOU7YMkfU/SpyLiV5K+LulISXPVeIX3pRY/d6ntEdsjo6OjeysBgK5ifgGoWqkAZfsANYbP7RFxtyRFxI6IeD0i3pB0k6Tj9vazEbE0IoYjYnhoaKiqvgGgFOYXgG5IBijblvQNSY9GxJebts9oKjtb0qbq2wOA9jG/AHRLmU/hnSBpvqSHbW8otn1G0rm256rx0eAtki7rQn8A0AnmF4CuKPMpvAckeS931WrNFAATD/MLQLewEjkAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmAhQAAEAmR0TvDmaPSnq6adM0Sc/3rIHq1LFveu6dOvbdzZ6PiIihLu27Z/YyvyT+v+6VOvYs1bNven6zlvOrpwHqtw5uj0TEcN8aaFMd+6bn3qlj33XseRDU8XGj596pY9/0XB5v4QEAAGQiQAEAAGTqd4Ba2ufjt6uOfdNz79Sx7zr2PAjq+LjRc+/UsW96Lqmv10ABAADUUb/PQAEAANRO3wKU7Y/Y3mz7CdvX9KuPHLa32H7Y9gbbI/3upxXbt9jeaXtT07aptlfafrz4emg/exyrRc+LbW8rHu8Ntj/azx7Hsj3L9n22f277EdufLLYP7GM9Ts8D/VgPmjrOL6keM4z51Rt1nF/SYM2wvryFZ3uSpP8l6Y8lbZW0TtK5EfHznjeTwfYWScMRMdBrZNg+SdKvJd0WEccU266XtCsilhQD/9CI+HQ/+2zWoufFkn4dEX/Tz95asT1D0oyI+KntKZLWSzpL0oUa0Md6nJ4/rgF+rAdJXeeXVI8ZxvzqjTrOL2mwZli/zkAdJ+mJiHgqIl6R9G1JZ/apl31ORKyRtGvM5jMl3VrcvlWNf3ADo0XPAy0itkfET4vbuyU9KmmmBvixHqdnlMf86iLmV2/UcX5JgzXD+hWgZkr6RdP3W1WPIR6Sfmh7ve1L+91MpukRsb24/Zyk6f1sJsMVtjcWp8gH6lRyM9uzJR0raa1q8liP6VmqyWM9AOo6v6T6zrBaPKf2ohbPqTrOL6n/M4yLyPOcGBHvkfQnki4vTtvWTjTet63Dxy+/LulISXMlbZf0pb5204LtgyR9T9KnIuJXzfcN6mO9l55r8VijY7WfYYP6nNqLWjyn6ji/pMGYYf0KUNskzWr6/veKbQMtIrYVX3dKWq7Gqfy62FG8d7znPeSdfe4nKSJ2RMTrEfGGpJs0gI+37QPUeBLfHhF3F5sH+rHeW891eKwHSC3nl1TrGTbQz6m9qcNzqo7zSxqcGdavALVO0lG2f9/2gZLOkXRPn3opxfbk4oI12Z4s6TRJm8b/qYFyj6QLitsXSPp+H3spZc+TuHC2Buzxtm1J35D0aER8uemugX2sW/U86I/1gKnd/JJqP8MG9jnVyqA/p+o4v6TBmmF9W0iz+Ijh30qaJOmWiPh8XxopyfYfqPGKTZL2l3THoPZs+05J89T4C9U7JF0n6e8lfVfS4Wr8RfmPR8TAXPTYoud5apyODUlbJF3W9N5839k+UdL9kh6W9Eax+TNqvB8/kI/1OD2fqwF+rAdN3eaXVJ8ZxvzqjTrOL2mwZhgrkQMAAGTiInIAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBMBCgAAIBM/w8o/F0cTQ9+NgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize before and after dropout\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(10,20)\n",
    "\n",
    "example = X_train[0].reshape(28,28)\n",
    "example_dropout = X_train_dropout[0].reshape(28,28)\n",
    "axs[0].imshow(example, cmap = mpl.cm.binary)\n",
    "axs[0].set_title('No Dropout')\n",
    "axs[1].imshow(example_dropout, cmap = mpl.cm.binary)\n",
    "axs[1].set_title('With Dropout')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Use Functional API to build a wide model that takes in three inputs. The first input will go through 5 layers. The second input will go through the last 2 layers. The final input will be fed through to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    # inputs\n",
    "    input_wide = keras.layers.Input(shape = [input_shape], name = 'wide')\n",
    "    input_shallow = keras.layers.Input(shape = [input_shape], name = 'shallow')\n",
    "    input_deep = keras.layers.Input(shape = [input_shape], name = 'deep')\n",
    "\n",
    "    # layers\n",
    "    hidden1 = keras.layers.Dense(256, activation = 'relu')(input_deep)\n",
    "    hidden2 = keras.layers.Dense(256, activation = 'relu')(hidden1)\n",
    "    hidden3 = keras.layers.Dense(256, activation = 'relu')(hidden2)\n",
    "    # concat with input shallow\n",
    "    concat_1 = keras.layers.concatenate([input_shallow, hidden3])\n",
    "    hidden4 = keras.layers.Dense(256, activation = 'relu')(concat_1)\n",
    "    hidden_final = keras.layers.Dense(256, activation = 'relu')(hidden4)\n",
    "    # concat with final deep input\n",
    "    concat_final = keras.layers.concatenate([input_wide, hidden_final])\n",
    "    output = keras.layers.Dense(10, name = 'output', activation = 'softmax')(concat_final)\n",
    "    model = keras.Model(inputs = [input_deep, input_shallow, input_wide], outputs = [output])\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.SGD(learning_rate = 1e-3), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "my_nn = build_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep (InputLayer)              [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          200960      ['deep[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " shallow (InputLayer)           [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          65792       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1040)         0           ['shallow[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          266496      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " wide (InputLayer)              [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          65792       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1040)         0           ['wide[0][0]',                   \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 10)           10410       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 675,242\n",
      "Trainable params: 675,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a validation set from training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_dropout, y_train, train_size = 0.75, random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 2.1875 - accuracy: 0.2203 - val_loss: 1.9592 - val_accuracy: 0.4095\n",
      "Epoch 2/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 1.7824 - accuracy: 0.5466 - val_loss: 1.6381 - val_accuracy: 0.6316\n",
      "Epoch 3/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 1.5079 - accuracy: 0.6855 - val_loss: 1.4074 - val_accuracy: 0.7133\n",
      "Epoch 4/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 1.3095 - accuracy: 0.7416 - val_loss: 1.2386 - val_accuracy: 0.7514\n",
      "Epoch 5/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 1.1631 - accuracy: 0.7720 - val_loss: 1.1121 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 1.0521 - accuracy: 0.7917 - val_loss: 1.0147 - val_accuracy: 0.7949\n",
      "Epoch 7/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.9656 - accuracy: 0.8050 - val_loss: 0.9377 - val_accuracy: 0.8067\n",
      "Epoch 8/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.8966 - accuracy: 0.8163 - val_loss: 0.8753 - val_accuracy: 0.8165\n",
      "Epoch 9/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.8403 - accuracy: 0.8231 - val_loss: 0.8239 - val_accuracy: 0.8229\n",
      "Epoch 10/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.7935 - accuracy: 0.8294 - val_loss: 0.7806 - val_accuracy: 0.8290\n",
      "Epoch 11/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.7539 - accuracy: 0.8354 - val_loss: 0.7439 - val_accuracy: 0.8341\n",
      "Epoch 12/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.7200 - accuracy: 0.8398 - val_loss: 0.7121 - val_accuracy: 0.8386\n",
      "Epoch 13/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.6907 - accuracy: 0.8436 - val_loss: 0.6845 - val_accuracy: 0.8417\n",
      "Epoch 14/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.8471 - val_loss: 0.6602 - val_accuracy: 0.8452\n",
      "Epoch 15/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.8500 - val_loss: 0.6385 - val_accuracy: 0.8483\n",
      "Epoch 16/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.8532 - val_loss: 0.6192 - val_accuracy: 0.8511\n",
      "Epoch 17/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.8555 - val_loss: 0.6017 - val_accuracy: 0.8549\n",
      "Epoch 18/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5874 - accuracy: 0.8582 - val_loss: 0.5859 - val_accuracy: 0.8567\n",
      "Epoch 19/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5724 - accuracy: 0.8597 - val_loss: 0.5716 - val_accuracy: 0.8588\n",
      "Epoch 20/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5588 - accuracy: 0.8620 - val_loss: 0.5584 - val_accuracy: 0.8604\n",
      "Epoch 21/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5463 - accuracy: 0.8639 - val_loss: 0.5463 - val_accuracy: 0.8619\n",
      "Epoch 22/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5348 - accuracy: 0.8657 - val_loss: 0.5352 - val_accuracy: 0.8633\n",
      "Epoch 23/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5241 - accuracy: 0.8676 - val_loss: 0.5248 - val_accuracy: 0.8651\n",
      "Epoch 24/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5142 - accuracy: 0.8690 - val_loss: 0.5153 - val_accuracy: 0.8668\n",
      "Epoch 25/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.5050 - accuracy: 0.8709 - val_loss: 0.5062 - val_accuracy: 0.8683\n",
      "Epoch 26/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4964 - accuracy: 0.8724 - val_loss: 0.4979 - val_accuracy: 0.8695\n",
      "Epoch 27/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4883 - accuracy: 0.8738 - val_loss: 0.4901 - val_accuracy: 0.8710\n",
      "Epoch 28/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4807 - accuracy: 0.8752 - val_loss: 0.4827 - val_accuracy: 0.8720\n",
      "Epoch 29/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4736 - accuracy: 0.8765 - val_loss: 0.4758 - val_accuracy: 0.8733\n",
      "Epoch 30/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4668 - accuracy: 0.8775 - val_loss: 0.4694 - val_accuracy: 0.8741\n",
      "Epoch 31/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4605 - accuracy: 0.8784 - val_loss: 0.4630 - val_accuracy: 0.8759\n",
      "Epoch 32/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4545 - accuracy: 0.8798 - val_loss: 0.4572 - val_accuracy: 0.8763\n",
      "Epoch 33/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4487 - accuracy: 0.8809 - val_loss: 0.4516 - val_accuracy: 0.8778\n",
      "Epoch 34/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4433 - accuracy: 0.8818 - val_loss: 0.4463 - val_accuracy: 0.8789\n",
      "Epoch 35/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4381 - accuracy: 0.8828 - val_loss: 0.4414 - val_accuracy: 0.8804\n",
      "Epoch 36/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4331 - accuracy: 0.8838 - val_loss: 0.4365 - val_accuracy: 0.8812\n",
      "Epoch 37/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4284 - accuracy: 0.8847 - val_loss: 0.4320 - val_accuracy: 0.8824\n",
      "Epoch 38/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4239 - accuracy: 0.8857 - val_loss: 0.4275 - val_accuracy: 0.8831\n",
      "Epoch 39/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4195 - accuracy: 0.8866 - val_loss: 0.4234 - val_accuracy: 0.8837\n",
      "Epoch 40/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4154 - accuracy: 0.8874 - val_loss: 0.4194 - val_accuracy: 0.8843\n",
      "Epoch 41/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4114 - accuracy: 0.8882 - val_loss: 0.4155 - val_accuracy: 0.8852\n",
      "Epoch 42/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4075 - accuracy: 0.8893 - val_loss: 0.4117 - val_accuracy: 0.8857\n",
      "Epoch 43/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4038 - accuracy: 0.8904 - val_loss: 0.4083 - val_accuracy: 0.8867\n",
      "Epoch 44/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.4002 - accuracy: 0.8914 - val_loss: 0.4049 - val_accuracy: 0.8873\n",
      "Epoch 45/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3968 - accuracy: 0.8917 - val_loss: 0.4015 - val_accuracy: 0.8888\n",
      "Epoch 46/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3935 - accuracy: 0.8924 - val_loss: 0.3983 - val_accuracy: 0.8888\n",
      "Epoch 47/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3903 - accuracy: 0.8933 - val_loss: 0.3952 - val_accuracy: 0.8898\n",
      "Epoch 48/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3872 - accuracy: 0.8940 - val_loss: 0.3922 - val_accuracy: 0.8903\n",
      "Epoch 49/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3841 - accuracy: 0.8947 - val_loss: 0.3894 - val_accuracy: 0.8910\n",
      "Epoch 50/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3812 - accuracy: 0.8955 - val_loss: 0.3866 - val_accuracy: 0.8915\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = my_nn.fit([X_train, X_train, X_train], y_train, epochs = 50, validation_data = ([X_valid, X_valid, X_valid], y_valid),  batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got a pretty high accuracy score on the validation set. \n",
    "\n",
    "Next I would like to see if scaling the data would help improve the validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Standard Scaler to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, y_train = reshape_X_and_y(X_train, y_train)\n",
    "X_test, y_test = reshape_X_and_y(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tr = X_train / 255.0\n",
    "X_test_tr = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize before and after scaling\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(10,20)\n",
    "\n",
    "example = X_train[0].reshape(28,28)\n",
    "example_tr = X_train_tr[0].reshape(28,28)\n",
    "axs[0].imshow(example, cmap = mpl.cm.binary)\n",
    "axs[0].set_title('Not Scaled')\n",
    "axs[1].imshow(example_tr, cmap = mpl.cm.binary)\n",
    "axs[1].set_title('Scaled')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_tr, y_train, train_size = 0.75, random_state = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "my_model2 = build_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3886 - accuracy: 0.8990 - val_loss: 0.3859 - val_accuracy: 0.8970\n",
      "Epoch 2/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3850 - accuracy: 0.8993 - val_loss: 0.3826 - val_accuracy: 0.8975\n",
      "Epoch 3/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3815 - accuracy: 0.9001 - val_loss: 0.3794 - val_accuracy: 0.8979\n",
      "Epoch 4/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3782 - accuracy: 0.9009 - val_loss: 0.3764 - val_accuracy: 0.8984\n",
      "Epoch 5/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3750 - accuracy: 0.9011 - val_loss: 0.3732 - val_accuracy: 0.8991\n",
      "Epoch 6/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3720 - accuracy: 0.9016 - val_loss: 0.3705 - val_accuracy: 0.8991\n",
      "Epoch 7/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3690 - accuracy: 0.9021 - val_loss: 0.3676 - val_accuracy: 0.8993\n",
      "Epoch 8/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3661 - accuracy: 0.9024 - val_loss: 0.3648 - val_accuracy: 0.9005\n",
      "Epoch 9/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3633 - accuracy: 0.9033 - val_loss: 0.3623 - val_accuracy: 0.9007\n",
      "Epoch 10/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3606 - accuracy: 0.9035 - val_loss: 0.3597 - val_accuracy: 0.9015\n",
      "Epoch 11/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3579 - accuracy: 0.9040 - val_loss: 0.3572 - val_accuracy: 0.9019\n",
      "Epoch 12/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3554 - accuracy: 0.9042 - val_loss: 0.3548 - val_accuracy: 0.9024\n",
      "Epoch 13/50\n",
      "352/352 [==============================] - 2s 5ms/step - loss: 0.3529 - accuracy: 0.9049 - val_loss: 0.3526 - val_accuracy: 0.9021\n",
      "Epoch 14/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3505 - accuracy: 0.9053 - val_loss: 0.3503 - val_accuracy: 0.9029\n",
      "Epoch 15/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3481 - accuracy: 0.9057 - val_loss: 0.3481 - val_accuracy: 0.9035\n",
      "Epoch 16/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3459 - accuracy: 0.9066 - val_loss: 0.3461 - val_accuracy: 0.9038\n",
      "Epoch 17/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3436 - accuracy: 0.9067 - val_loss: 0.3438 - val_accuracy: 0.9041\n",
      "Epoch 18/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3414 - accuracy: 0.9070 - val_loss: 0.3418 - val_accuracy: 0.9041\n",
      "Epoch 19/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3393 - accuracy: 0.9077 - val_loss: 0.3397 - val_accuracy: 0.9045\n",
      "Epoch 20/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9080 - val_loss: 0.3378 - val_accuracy: 0.9053\n",
      "Epoch 21/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3352 - accuracy: 0.9084 - val_loss: 0.3357 - val_accuracy: 0.9055\n",
      "Epoch 22/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3332 - accuracy: 0.9088 - val_loss: 0.3340 - val_accuracy: 0.9061\n",
      "Epoch 23/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.9095 - val_loss: 0.3322 - val_accuracy: 0.9061\n",
      "Epoch 24/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3293 - accuracy: 0.9096 - val_loss: 0.3303 - val_accuracy: 0.9067\n",
      "Epoch 25/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3274 - accuracy: 0.9100 - val_loss: 0.3285 - val_accuracy: 0.9069\n",
      "Epoch 26/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3255 - accuracy: 0.9104 - val_loss: 0.3268 - val_accuracy: 0.9077\n",
      "Epoch 27/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3237 - accuracy: 0.9107 - val_loss: 0.3251 - val_accuracy: 0.9079\n",
      "Epoch 28/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3220 - accuracy: 0.9111 - val_loss: 0.3234 - val_accuracy: 0.9084\n",
      "Epoch 29/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3202 - accuracy: 0.9113 - val_loss: 0.3217 - val_accuracy: 0.9091\n",
      "Epoch 30/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3185 - accuracy: 0.9116 - val_loss: 0.3200 - val_accuracy: 0.9095\n",
      "Epoch 31/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3168 - accuracy: 0.9121 - val_loss: 0.3185 - val_accuracy: 0.9091\n",
      "Epoch 32/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3151 - accuracy: 0.9123 - val_loss: 0.3170 - val_accuracy: 0.9097\n",
      "Epoch 33/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3135 - accuracy: 0.9129 - val_loss: 0.3154 - val_accuracy: 0.9100\n",
      "Epoch 34/50\n",
      "352/352 [==============================] - 2s 5ms/step - loss: 0.3118 - accuracy: 0.9130 - val_loss: 0.3139 - val_accuracy: 0.9100\n",
      "Epoch 35/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3103 - accuracy: 0.9137 - val_loss: 0.3124 - val_accuracy: 0.9103\n",
      "Epoch 36/50\n",
      "352/352 [==============================] - 2s 5ms/step - loss: 0.3086 - accuracy: 0.9140 - val_loss: 0.3109 - val_accuracy: 0.9107\n",
      "Epoch 37/50\n",
      "352/352 [==============================] - 2s 5ms/step - loss: 0.3071 - accuracy: 0.9146 - val_loss: 0.3094 - val_accuracy: 0.9110\n",
      "Epoch 38/50\n",
      "352/352 [==============================] - 2s 4ms/step - loss: 0.3056 - accuracy: 0.9147 - val_loss: 0.3080 - val_accuracy: 0.9113\n",
      "Epoch 39/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3041 - accuracy: 0.9155 - val_loss: 0.3067 - val_accuracy: 0.9117\n",
      "Epoch 40/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.9157 - val_loss: 0.3052 - val_accuracy: 0.9120\n",
      "Epoch 41/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.3011 - accuracy: 0.9157 - val_loss: 0.3038 - val_accuracy: 0.9123\n",
      "Epoch 42/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.9161 - val_loss: 0.3024 - val_accuracy: 0.9125\n",
      "Epoch 43/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2982 - accuracy: 0.9167 - val_loss: 0.3011 - val_accuracy: 0.9133\n",
      "Epoch 44/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2968 - accuracy: 0.9169 - val_loss: 0.2998 - val_accuracy: 0.9135\n",
      "Epoch 45/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2954 - accuracy: 0.9175 - val_loss: 0.2984 - val_accuracy: 0.9143\n",
      "Epoch 46/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2940 - accuracy: 0.9177 - val_loss: 0.2972 - val_accuracy: 0.9137\n",
      "Epoch 47/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2926 - accuracy: 0.9181 - val_loss: 0.2959 - val_accuracy: 0.9143\n",
      "Epoch 48/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2913 - accuracy: 0.9182 - val_loss: 0.2945 - val_accuracy: 0.9148\n",
      "Epoch 49/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2899 - accuracy: 0.9185 - val_loss: 0.2933 - val_accuracy: 0.9149\n",
      "Epoch 50/50\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.2886 - accuracy: 0.9189 - val_loss: 0.2921 - val_accuracy: 0.9151\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = my_nn.fit([X_train, X_train, X_train], y_train, epochs = 50, validation_data = ([X_valid, X_valid, X_valid], y_valid),  batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the scaled dataset performed better than the dropout set. We will use the scaled dataset and try to find the optimal parameters for the model.\n",
    "\n",
    "Let's use a callback to find the best learning rate for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is taken from [the hands on machine learning repository](https://github.com/knolasco/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "my_nn_w_callback = build_model(X_train.shape[1])\n",
    "exp_lr = ExponentialLearningRate(factor = 1.005) # increase by 0.5% each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "176/176 [==============================] - 2s 8ms/step - loss: 2.2736 - accuracy: 0.1542 - val_loss: 2.0637 - val_accuracy: 0.3291\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 1.8097 - accuracy: 0.5501 - val_loss: 1.5279 - val_accuracy: 0.7059\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 1.2348 - accuracy: 0.7705 - val_loss: 0.9652 - val_accuracy: 0.8171\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 0.7624 - accuracy: 0.8398 - val_loss: 0.5975 - val_accuracy: 0.8621\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 0.4888 - accuracy: 0.8763 - val_loss: 0.4020 - val_accuracy: 0.8879\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 1s 8ms/step - loss: 0.3485 - accuracy: 0.9032 - val_loss: 0.2988 - val_accuracy: 0.9149\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 0.2806 - accuracy: 0.9182 - val_loss: 0.3738 - val_accuracy: 0.8787\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 0.2710 - accuracy: 0.9205 - val_loss: 0.1966 - val_accuracy: 0.9381\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 1s 8ms/step - loss: 0.4955 - accuracy: 0.8833 - val_loss: 0.3967 - val_accuracy: 0.8913\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 2s 9ms/step - loss: nan - accuracy: 0.1575 - val_loss: nan - val_accuracy: 0.0963\n"
     ]
    }
   ],
   "source": [
    "history = my_nn_w_callback.fit([X_train, X_train, X_train], y_train, epochs = 10, validation_data = ([X_valid, X_valid, X_valid], y_valid), callbacks = [exp_lr], batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the loss against the learning rate to find the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(exp_lr.rates, exp_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(exp_lr.losses), min(exp_lr.rates), max(exp_lr.rates))\n",
    "plt.axis([min(exp_lr.rates), max(exp_lr.rates), 0, exp_lr.losses[0]])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(exp_lr.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min index for where the minimum loss occurs\n",
    "val, idx = min((val, idx) for (idx, val) in enumerate(exp_lr.losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16867539286613464, 1.1215323)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val, exp_lr.rates[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another model with the same structure but the optimal learning rate. We will also use an early stopping call back and prepare the model to use tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_optimized(input_shape, best_lr):\n",
    "    # inputs\n",
    "    input_wide = keras.layers.Input(shape = [input_shape], name = 'wide')\n",
    "    input_shallow = keras.layers.Input(shape = [input_shape], name = 'shallow')\n",
    "    input_deep = keras.layers.Input(shape = [input_shape], name = 'deep')\n",
    "\n",
    "    # layers\n",
    "    hidden1 = keras.layers.Dense(256, activation = 'relu')(input_deep)\n",
    "    hidden2 = keras.layers.Dense(256, activation = 'relu')(hidden1)\n",
    "    hidden3 = keras.layers.Dense(256, activation = 'relu')(hidden2)\n",
    "    # concat with input shallow\n",
    "    concat_1 = keras.layers.concatenate([input_shallow, hidden3])\n",
    "    hidden4 = keras.layers.Dense(256, activation = 'relu')(concat_1)\n",
    "    hidden_final = keras.layers.Dense(256, activation = 'relu')(hidden4)\n",
    "    # concat with final deep input\n",
    "    concat_final = keras.layers.concatenate([input_wide, hidden_final])\n",
    "    output = keras.layers.Dense(10, name = 'output', activation = 'softmax')(concat_final)\n",
    "    model = keras.Model(inputs = [input_deep, input_shallow, input_wide], outputs = [output])\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.SGD(learning_rate = best_lr), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "best_model = build_model_optimized(X_train.shape[1], best_lr = exp_lr.rates[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_mnist_logs\\\\run_001'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "run_index = 1 # increment this at every run\n",
    "run_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.3979 - accuracy: 0.8927 - val_loss: 0.2618 - val_accuracy: 0.9245\n",
      "Epoch 2/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.1855 - accuracy: 0.9475 - val_loss: 0.1854 - val_accuracy: 0.9502\n",
      "Epoch 3/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.1342 - accuracy: 0.9624 - val_loss: 0.1518 - val_accuracy: 0.9591\n",
      "Epoch 4/75\n",
      "704/704 [==============================] - 2s 4ms/step - loss: 0.0972 - accuracy: 0.9714 - val_loss: 0.1177 - val_accuracy: 0.9696\n",
      "Epoch 5/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9756 - val_loss: 1.8977 - val_accuracy: 0.7167\n",
      "Epoch 6/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9771 - val_loss: 0.1293 - val_accuracy: 0.9678\n",
      "Epoch 7/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0527 - accuracy: 0.9838 - val_loss: 0.6006 - val_accuracy: 0.9133\n",
      "Epoch 8/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0499 - accuracy: 0.9850 - val_loss: 0.1197 - val_accuracy: 0.9729\n",
      "Epoch 9/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0416 - accuracy: 0.9875 - val_loss: 0.1234 - val_accuracy: 0.9726\n",
      "Epoch 10/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0309 - accuracy: 0.9899 - val_loss: 0.1244 - val_accuracy: 0.9743\n",
      "Epoch 11/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.1254 - val_accuracy: 0.9737\n",
      "Epoch 12/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.1789 - val_accuracy: 0.9656\n",
      "Epoch 13/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0184 - accuracy: 0.9936 - val_loss: 0.1177 - val_accuracy: 0.9769\n",
      "Epoch 14/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.1333 - val_accuracy: 0.9733\n",
      "Epoch 15/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.1181 - val_accuracy: 0.9784\n",
      "Epoch 16/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.1088 - val_accuracy: 0.9800\n",
      "Epoch 17/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.1062 - val_accuracy: 0.9810\n",
      "Epoch 18/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 4.3649e-04 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9811\n",
      "Epoch 19/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.7527e-04 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9813\n",
      "Epoch 20/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 2.0261e-04 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9815\n",
      "Epoch 21/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.6769e-04 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9815\n",
      "Epoch 22/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.4932e-04 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 0.9815\n",
      "Epoch 23/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.3353e-04 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9817\n",
      "Epoch 24/75\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 1.2053e-04 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9820\n",
      "Epoch 25/75\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 1.0938e-04 - accuracy: 1.0000 - val_loss: 0.1111 - val_accuracy: 0.9819\n",
      "Epoch 26/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.0124e-04 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9820\n",
      "Epoch 27/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 9.4130e-05 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9821\n",
      "Epoch 28/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 8.7882e-05 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9821\n",
      "Epoch 29/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 8.2818e-05 - accuracy: 1.0000 - val_loss: 0.1131 - val_accuracy: 0.9821\n",
      "Epoch 30/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 7.8049e-05 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9820\n",
      "Epoch 31/75\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 7.3680e-05 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9822\n",
      "Epoch 32/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 7.0037e-05 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9821\n",
      "Epoch 33/75\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 6.6666e-05 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 15)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_mnist_best_model.h5', save_best_only = True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = best_model.fit([X_train, X_train, X_train], y_train, epochs = 75, \n",
    "                        validation_data = ([X_valid, X_valid, X_valid], y_valid), batch_size = 64, callbacks = [early_stopping_cb, checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model stopped training after 33 of the 75 epochs. Let's see how it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_mnist_best_model.h5\") # rollback to best model\n",
    "y_test_preds = model.predict([X_test_tr, X_test_tr, X_test_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 834us/step - loss: 0.0965 - accuracy: 0.9812\n",
      "Accuracy on test set: 98.12%\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate([X_test_tr, X_test_tr, X_test_tr], y_test)\n",
    "\n",
    "print('Accuracy on test set: {:.2%}'.format(acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_mnist_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From the tensorboard results (not shown here), we can see that the training accuracy leveled off at 100%. Although this is typically a sign of overfitting, we can see that the validation accuracy converged nicely to 98.23% when training the model. Moreover, this accuracy translated very well with the test set and we ended with a model that is 98.12% accurate in data that it has never seen before.\n",
    "\n",
    "There were many take aways from this module. I struggled to understand what callbacks are and how to use them the first time I encountered them, but I feel like I have a much better understanding of them now. I will continue to learn tensorflow and apply it to problems that I encounter in the future.\n",
    "\n",
    "Thank you for reading."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32119642d05386763e8c8562dea81f2d1a47658af5751929f873a465c4a1c96a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
